# **Neural Turing Machines**
- - -
An implementation of *Neural Turing Machines* in **mlx** as described by arXiv:1410.5401v2.
> We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.

More than anything, I am now convinced, NTMs speak to the will of their creators to somehow manage the ungodly gradient dynamics that emerge when training them, especially when dealing with the feed-forward variant—a challenge worsened by the scarcity of open-source implementations and, more surprisingly, by the absence of substantial literature investigating them. Many hours were spent, nonetheless, modifying the architecture and training loop in the hopes replicating the original paper’s findings. This aim, however, was not achieved—partly due to serendipitous gradient collapses to NaN, and partly due to insufficient access to compute resources. It does feel odd, however, that not much is spoken about them, specially given how intutive they seem as a natural extension of regular FFNNs.

Implementation-wise, the only notable deviation from standard implementations, to the extent that standard implemnations exists, is the use of a branching architecture, where both the controller and output branch are isolated from one another and in turn share a common preprocessing module. That being said, I have abstained from proving any detailed decription of architectural detials since, 1) most can be inferred from the code and follow the papers instrction and 2) LLMs make any attempt at a techinical report redundant.
